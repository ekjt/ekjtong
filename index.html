<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Edmond Tong</title>
  <meta name="author" content="Edmond Tong">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="stylesheet.css">
</head>

<body>
  <table style="width:100%; max-width:800px; margin:auto; border:0; border-spacing:0;">
    <tr>
      <td>
        <!-- Header Section -->
        <table style="width:100%; margin:auto; border:0; border-spacing:0;">
          <tr>
            <td style="padding:2.5%; width:63%; vertical-align:middle;">
              <p class="name" style="text-align:center;">Edmond Tong</p>
              <p>I am currently pursuing a PhD in Robotics at Queensland University of Technology (QUT) under the supervision of Prof. Niko Sünderhauf and Prof. Jonathan Roberts. I completed an MS in Robotics from the University of Michigan (2024), and I hold a BS in Mechanical Engineering from Brigham Young University (2018). My research interests lie at the intersection of computer vision, robotics, and machine learning.</p>
              <p>I am interested in advancing robotic capabilities by exploring visual and semantic representations derived from human video demonstrations. Specifically, my goal is to enable robots to learn complex, manipulation policies by efficiently extracting skills, affordances, logic from unstructured human-centric data, thereby allowing for robust policy generalization across diverse, real-world environments.</p>              <p style="text-align:center;">
                <a href="mailto:ekjtong@gmail.com">Email</a> 
                <!-- &nbsp;/&nbsp;
                <a href="data/EdmondCV.pdf">CV</a> -->
              </p>
            </td>
            <td style="padding:2.5%; width:37%; max-width:40%;">
              <a href="images/profile.jpg">
                <img src="images/profile.jpg" alt="profile photo" style="width:100%; border-radius:50%; object-fit:cover;">
              </a>
            </td>
          </tr>
        </table>

        <!-- Research Section -->
        <table style="width:100%; margin:auto; border:0; border-spacing:0;">
          <tr>
            <td style="padding:20px;">
              <h2>Research</h2>
            </td>
          </tr>
        </table>

        <!-- Project Entry -->
        <table style="width:100%; margin:auto; border:0; border-spacing:0;">
          <tr>
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <img src="images/grasp.png" alt="OVAL-Grasp" width="160" height="160">
            </td>
            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://ekjt.github.io/OVAL-Grasp/">
                <span class="papertitle"> OVAL‑Grasp: Open‑Vocabulary Affordance Localization for Task Oriented Grasping </span>
              </a>
              <br>
              <strong>Edmond Tong</strong>, Advaith Balaji, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins
              <br>
              <em> ISER </em> 2025
              <br>
              <a href="https://ekjt.github.io/OVAL-Grasp/">project page</a> /
              <a href="https://youtu.be/M2PrpD1ZlSI">video</a> /
              <a href="https://arxiv.org/abs/2511.20841">arXiv</a>
              <p>Utilizing large language models to associate affordances with specific parts of objects, and ground them using a VLM to generate task oriented grasp.</p>
            </td>
          </tr>          
          <tr>
            <td style="padding:20px; width:25%; vertical-align:middle;">
              <img src="images/oval.png" alt="OVAL-Prompt" width="160" height="160">
            </td>
            <td style="padding:20px; width:75%; vertical-align:middle;">
              <a href="https://ekjt.github.io/OVAL-Prompt/">
                <span class="papertitle">OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding</span>
              </a>
              <br>
              <strong>Edmond Tong</strong>, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins
              <br>
              <em>ICRA VLMNM Workshop</em>, 2024
              <br>
              <a href="https://ekjt.github.io/OVAL-Prompt/">project page</a> /
              <a href="https://youtu.be/ZQpyqRcAjrU">video</a> /
              <a href="https://arxiv.org/abs/2404.11000">arXiv</a>
              <p>Utilizing large language models to associate affordances with specific parts of objects, thereby localizing them to facilitate manipulation.</p>
            </td>
          </tr>

        </table>

        <!-- Footer -->
        <table style="width:100%; margin:auto; border:0; border-spacing:0;">
          <tr>
            <td>
              <br>
              <p style="text-align:right; font-size:small;"></p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>
</html>
